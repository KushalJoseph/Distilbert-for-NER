# Distilbert-for-NER
Using a larger BERT-based model for the NER task, this project "distils" the knowledge into a smaller model, thereby providing similar accuracy levels, but fewer model parameters
